{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1192499,"sourceType":"datasetVersion","datasetId":622510}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/chenzhijing3121/finetune-finbert-for-financial-sentiment-analysis?scriptVersionId=267546762\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-12T16:27:23.666853Z","iopub.execute_input":"2025-10-12T16:27:23.66707Z","iopub.status.idle":"2025-10-12T16:27:25.027439Z","shell.execute_reply.started":"2025-10-12T16:27:23.667042Z","shell.execute_reply":"2025-10-12T16:27:25.026646Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/sentiment-analysis-for-financial-news/all-data.csv\n/kaggle/input/sentiment-analysis-for-financial-news/FinancialPhraseBank/Sentences_66Agree.txt\n/kaggle/input/sentiment-analysis-for-financial-news/FinancialPhraseBank/Sentences_AllAgree.txt\n/kaggle/input/sentiment-analysis-for-financial-news/FinancialPhraseBank/README.txt\n/kaggle/input/sentiment-analysis-for-financial-news/FinancialPhraseBank/License.txt\n/kaggle/input/sentiment-analysis-for-financial-news/FinancialPhraseBank/Sentences_75Agree.txt\n/kaggle/input/sentiment-analysis-for-financial-news/FinancialPhraseBank/Sentences_50Agree.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ===================== 0) Setup (Kaggle) =====================\n!pip -q install -U transformers peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T16:27:25.029229Z","iopub.execute_input":"2025-10-12T16:27:25.029519Z","iopub.status.idle":"2025-10-12T16:28:56.015101Z","shell.execute_reply.started":"2025-10-12T16:27:25.029502Z","shell.execute_reply":"2025-10-12T16:28:56.014111Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.9/504.9 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ===================== 1) Imports & Env ======================\nimport os, warnings, numpy as np, pandas as pd, torch\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\nimport numpy as np\nimport pandas as pd\n\nimport transformers\nfrom transformers import (\n    AutoTokenizer, AutoModelForSequenceClassification,\n    TrainingArguments, Trainer\n)\nfrom peft import LoraConfig, get_peft_model\n\nprint(\"transformers==\", transformers.__version__)\nprint(\"torch==\", torch.__version__, \"| cuda:\", torch.cuda.is_available())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T16:28:56.016242Z","iopub.execute_input":"2025-10-12T16:28:56.016705Z","iopub.status.idle":"2025-10-12T16:29:19.973259Z","shell.execute_reply.started":"2025-10-12T16:28:56.016668Z","shell.execute_reply":"2025-10-12T16:29:19.972598Z"}},"outputs":[{"name":"stderr","text":"2025-10-12 16:29:06.985543: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760286547.149999      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760286547.197271      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"transformers== 4.57.0\ntorch== 2.6.0+cu124 | cuda: True\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ===================== 2) Model / Tokeniser ==================\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom peft import LoraConfig, get_peft_model, TaskType\n\nMODEL_NAME = \"ProsusAI/finbert\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\nid2label = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\nlabel2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n\ndef build_base_model():\n    return AutoModelForSequenceClassification.from_pretrained(\n        MODEL_NAME,\n        num_labels=3,\n        id2label=id2label,\n        label2id=label2id,\n    )\n\ndef build_lora_model():\n    base = build_base_model()\n    lora_cfg = LoraConfig(\n        task_type=TaskType.SEQ_CLS,\n        r=16, lora_alpha=32, lora_dropout=0.05,\n        target_modules=[\"query\",\"value\"],  \n        bias=\"none\",\n    )\n    return get_peft_model(base, lora_cfg)\ndef train_once(model, train_data, eval_data, out_dir, *, lr, bsz_train, bsz_eval, epochs):\n    args = TrainingArguments(\n        output_dir=out_dir,\n        learning_rate=lr,\n        per_device_train_batch_size=bsz_train,\n        per_device_eval_batch_size=bsz_eval,\n        num_train_epochs=epochs,\n        lr_scheduler_type=\"linear\",\n        optim=\"adamw_torch\",\n        logging_steps=100,\n        report_to=\"none\",\n        seed=42,\n        fp16=torch.cuda.is_available(),\n    )\n    trainer = Trainer(\n        model=model,\n        args=args,\n        train_dataset=train_data,\n        eval_dataset=eval_data,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics,  \n    )\n    trainer.train()\n    metrics_val = trainer.evaluate(eval_data)\n    return trainer, metrics_val\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T16:29:19.974Z","iopub.execute_input":"2025-10-12T16:29:19.974607Z","iopub.status.idle":"2025-10-12T16:29:20.965158Z","shell.execute_reply.started":"2025-10-12T16:29:19.974567Z","shell.execute_reply":"2025-10-12T16:29:20.964601Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fb2d1f3b7fb4a03b28a6dba953b6ab7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1875a08ea1c4e1e90dee27d2ff698af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be75d026526d4cbf91552f1542ff6cb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3174af4f5d8d45e292b2db25daf4bd29"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# ===================== 3) Load & Split Data ==================\nfilename = \"/kaggle/input/sentiment-analysis-for-financial-news/all-data.csv\"\ndf = pd.read_csv(\n    filename, names=[\"sentiment\", \"text\"],\n    encoding=\"ISO-8859-1\", engine=\"python\"\n)\n\nparts_train, parts_test = [], []\nfor s in [\"positive\", \"neutral\", \"negative\"]:\n    tr, te = train_test_split(\n        df[df.sentiment == s], train_size=300, test_size=300, random_state=42\n    )\n    parts_train.append(tr); parts_test.append(te)\n\nX_train = pd.concat(parts_train) \nX_test  = pd.concat(parts_test)\n\nused_idx = set(X_train.index) | set(X_test.index)\nX_eval = (df.loc[df.index.difference(used_idx)]\n            .groupby('sentiment', group_keys=False)\n            .apply(lambda x: x.sample(n=50, random_state=10, replace=True)))\n\nX_train = X_train.sample(frac=1, random_state=10).reset_index(drop=True)\nX_test  = X_test.reset_index(drop=True)\nX_eval  = X_eval.reset_index(drop=True)\n\nfor _df in [X_train, X_test, X_eval]:\n    _df[\"labels\"] = _df.sentiment.map(label2id).astype(\"int64\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T16:29:20.96583Z","iopub.execute_input":"2025-10-12T16:29:20.966069Z","iopub.status.idle":"2025-10-12T16:29:21.025248Z","shell.execute_reply.started":"2025-10-12T16:29:20.966051Z","shell.execute_reply":"2025-10-12T16:29:21.024746Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ===================== 4) Build HF Datasets ==================\ndef tok_fn(batch):\n    return tokenizer(\n        batch[\"text\"], padding=\"max_length\", truncation=True, max_length=256\n    )\n\ntrain_data = Dataset.from_pandas(X_train).map(tok_fn, batched=True)\\\n    .remove_columns([\"text\",\"sentiment\"])\ntest_data  = Dataset.from_pandas(X_test).map(tok_fn, batched=True)\\\n    .remove_columns([\"text\",\"sentiment\"])\neval_data  = Dataset.from_pandas(X_eval).map(tok_fn, batched=True)\\\n    .remove_columns([\"text\",\"sentiment\"])\n\ncols = [\"input_ids\", \"attention_mask\", \"labels\"]\ntrain_data.set_format(\"torch\", columns=cols)\ntest_data.set_format(\"torch\",  columns=cols)\neval_data.set_format(\"torch\",  columns=cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T16:29:21.025942Z","iopub.execute_input":"2025-10-12T16:29:21.026185Z","iopub.status.idle":"2025-10-12T16:29:21.576179Z","shell.execute_reply.started":"2025-10-12T16:29:21.026164Z","shell.execute_reply":"2025-10-12T16:29:21.575407Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b85d8f5816264a20b714ca27166a147a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe5740a143cc47278fa10b68dd4a8b9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ffa5787084f493394d50ebc85cd10a0"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# ===================== 5) Metrics ============================\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=1)\n    return {\n        \"f1\": f1_score(labels, preds, average=\"weighted\"),\n        \"accuracy\": accuracy_score(labels, preds),\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T16:29:21.577835Z","iopub.execute_input":"2025-10-12T16:29:21.578154Z","iopub.status.idle":"2025-10-12T16:29:21.582195Z","shell.execute_reply.started":"2025-10-12T16:29:21.578136Z","shell.execute_reply":"2025-10-12T16:29:21.581608Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# ===================== 6)Train Full & LoRA =====================\n# Full fine-tuning\nprint(\"🔹 Training FULL fine-tuning...\")\ntrainer_full, res_full = train_once(\n    model=build_base_model(),\n    train_data=train_data, eval_data=eval_data,\n    out_dir=f\"finetuned_full_{MODEL_NAME.split('/')[-1]}\",\n    lr=2e-5, bsz_train=8, bsz_eval=8, epochs=5\n)\nprint(f\"[FULL] val_f1={res_full['eval_f1']:.5f} | val_acc={res_full['eval_accuracy']:.5f}\")\n\n# LoRA fine-tuning\nprint(\"\\n🔹 Training LoRA fine-tuning...\")\ntrainer_lora, res_lora = train_once(\n    model=build_lora_model(),\n    train_data=train_data, eval_data=eval_data,\n    out_dir=f\"finetuned_lora_{MODEL_NAME.split('/')[-1]}\",\n    lr=1e-3, bsz_train=32, bsz_eval=64, epochs=5\n)\nprint(f\"[LoRA] val_f1={res_lora['eval_f1']:.5f} | val_acc={res_lora['eval_accuracy']:.5f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T16:29:21.582954Z","iopub.execute_input":"2025-10-12T16:29:21.583275Z","iopub.status.idle":"2025-10-12T16:32:44.813978Z","shell.execute_reply.started":"2025-10-12T16:29:21.583249Z","shell.execute_reply":"2025-10-12T16:32:44.81336Z"}},"outputs":[{"name":"stdout","text":"🔹 Training FULL fine-tuning...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d3158c5b4634ebb8d9b1144e9194adb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1153024b15f74af1a848e662d0f75f2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='565' max='565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [565/565 02:00, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.014200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.303900</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.124400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.070600</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.037500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"[FULL] val_f1=0.89767 | val_acc=0.90000\n\n🔹 Training LoRA fine-tuning...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='145' max='145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [145/145 01:14, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.318600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"[LoRA] val_f1=0.91185 | val_acc=0.91333\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ===================== 7)Test & Compare on hold-out test set =====================\ndef eval_trainer(trainer, test_data, name=\"MODEL\"):\n    pred = trainer.predict(test_data)\n    y_true = pred.label_ids\n    y_pred = np.argmax(pred.predictions, axis=1)\n    acc = accuracy_score(y_true, y_pred)\n    f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n    f1_weighted = f1_score(y_true, y_pred, average=\"weighted\")\n    print(f\"\\n===== {name} : Test Report =====\")\n    print(classification_report(y_true, y_pred, target_names=[id2label[i] for i in sorted(id2label)] , digits=4))\n    print(\"Confusion matrix:\")\n    print(confusion_matrix(y_true, y_pred, labels=sorted(id2label.keys())))\n    print(f\"Accuracy:    {acc:.4f}\")\n    print(f\"F1 (macro):  {f1_macro:.4f}\")\n    print(f\"F1 (weighted): {f1_weighted:.4f}\")\n    return {\"name\": name, \"accuracy\": acc, \"f1_macro\": f1_macro, \"f1_weighted\": f1_weighted}\n\nm_full = eval_trainer(trainer_full, test_data, name=\"FinBERT Full FT\")\nm_lora = eval_trainer(trainer_lora, test_data, name=\"FinBERT LoRA\")\n\ncmp_df = pd.DataFrame([m_full, m_lora]).set_index(\"name\")\nprint(\"\\n===== Summary (Test) =====\")\nprint(cmp_df.sort_values(\"f1_macro\", ascending=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T16:32:44.814925Z","iopub.execute_input":"2025-10-12T16:32:44.815206Z","iopub.status.idle":"2025-10-12T16:32:58.527156Z","shell.execute_reply.started":"2025-10-12T16:32:44.815182Z","shell.execute_reply":"2025-10-12T16:32:58.526509Z"}},"outputs":[{"name":"stdout","text":"\n===== FinBERT Full FT : Test Report =====\n              precision    recall  f1-score   support\n\n    negative     0.9262    0.9200    0.9231       300\n     neutral     0.8694    0.7767    0.8204       300\n    positive     0.8024    0.8933    0.8454       300\n\n    accuracy                         0.8633       900\n   macro avg     0.8660    0.8633    0.8630       900\nweighted avg     0.8660    0.8633    0.8630       900\n\nConfusion matrix:\n[[276  13  11]\n [ 12 233  55]\n [ 10  22 268]]\nAccuracy:    0.8633\nF1 (macro):  0.8630\nF1 (weighted): 0.8630\n\n===== FinBERT LoRA : Test Report =====\n              precision    recall  f1-score   support\n\n    negative     0.9412    0.9600    0.9505       300\n     neutral     0.8912    0.8467    0.8684       300\n    positive     0.8673    0.8933    0.8801       300\n\n    accuracy                         0.9000       900\n   macro avg     0.8999    0.9000    0.8997       900\nweighted avg     0.8999    0.9000    0.8997       900\n\nConfusion matrix:\n[[288   4   8]\n [ 13 254  33]\n [  5  27 268]]\nAccuracy:    0.9000\nF1 (macro):  0.8997\nF1 (weighted): 0.8997\n\n===== Summary (Test) =====\n                 accuracy  f1_macro  f1_weighted\nname                                            \nFinBERT LoRA     0.900000  0.899667     0.899667\nFinBERT Full FT  0.863333  0.862975     0.862975\n","output_type":"stream"}],"execution_count":9}]}